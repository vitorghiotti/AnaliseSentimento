{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import csv\n",
    "import nltk\n",
    "import unicodedata\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sqlite3\n",
    "from sqlite3 import Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSentence(sentence):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    sentence = sentence.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    sentence = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',sentence)\n",
    "    #Convert @username to AT_USER\n",
    "    sentence = re.sub('@[^\\s]+','AT_USER',sentence)\n",
    "    #Remove additional white spaces\n",
    "    sentence = re.sub('[\\s]+', ' ', sentence)\n",
    "    #Replace #word with word\n",
    "    sentence = re.sub(r'#([^\\s]+)', r'\\1', sentence)\n",
    "    #trimd\n",
    "    sentence = sentence.strip('\\'\"')\n",
    "    #retira açentos e caracteres especiais\n",
    "    sentence = str(unicodedata.normalize('NFKD', sentence).encode('ascii','ignore'),'utf8')\n",
    "    #************************* \n",
    "    sentence = sentence.replace(\"tmb\", \"tambem\")\n",
    "    sentence = sentence.replace(\" ta \", \" esta \")\n",
    "    sentence = sentence.replace(\"tbm\", \"tambem\")\n",
    "    sentence = sentence.replace(\"vc\", \"voce\")\n",
    "    sentence = sentence.replace(\"sdd\", \"saudade\")\n",
    "    sentence = sentence.replace(\"sdds\", \"saudade\")\n",
    "    sentence = sentence.replace(\"dnv\", \"novamente\")\n",
    "    sentence = sentence.replace(\"pfvr\", \"por favor\")\n",
    "    sentence = sentence.replace(\"/\", \" \")\n",
    "    sentence = sentence.replace(\"numca\", \"nunca\")\n",
    "    sentence = sentence.replace(\"magico\", \"magica\")\n",
    "    sentence = sentence.replace(\"lindo\", \"linda\")\n",
    "    sentence = sentence.replace(\"amore\", \"amor\")\n",
    "    sentence = sentence.replace(\" n \", \" nao \")\n",
    "    sentence = sentence.replace(\"ambas\", \"ambos\")\n",
    "    sentence = sentence.replace(\"tjm\", \"estamos juntos\")\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize stopWords\n",
    "stopWords = []\n",
    "\n",
    "#start replaceTwoOrMore\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    #return pattern.sub(r\"\\1\\1\", s)\n",
    "    return pattern.sub(r\"\\1\", s)\n",
    "#end\n",
    "\n",
    "#start getStopWordList\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    \n",
    "    if 1==1:\n",
    "        stopWords = nltk.corpus.stopwords.words('portuguese')\n",
    "    else:\n",
    "        fp = open(stopWordListFileName, 'r')\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            word = line.strip()\n",
    "            stopWords.append(word)\n",
    "            line = fp.readline()\n",
    "        fp.close()\n",
    "        \n",
    "    #****************************************\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "    stopWords.append('url')\n",
    "    stopWords.append('pra')\n",
    "    stopWords.append('q')\n",
    "    stopWords.append('at_user')\n",
    "    stopWords.append('poder')\n",
    "    stopWords.append('profunda')\n",
    "    stopWords.append('voce')\n",
    "    stopWords.append('to')    \n",
    "    stopWords.append('tao') \n",
    "    #*****************************************\n",
    "    stopWords.remove('nem')\n",
    "    #stopWords.remove('cabeca')\n",
    "    #stopWords.remove('cabeça')\n",
    "    #*****************************************\n",
    "            \n",
    "    return stopWords\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start getfeatureVector\n",
    "def getFeatureVector(sentence,stopWords):\n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = sentence.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.!')\n",
    "        if len(w)>1:\n",
    "            if 1!=1:\n",
    "                #print(len(w))\n",
    "                aux = w\n",
    "                stemmer = nltk.stem.SnowballStemmer('portuguese')\n",
    "                w = stemmer.stem(w)\n",
    "                #if w=='indo':\n",
    "                    #print(aux)\n",
    "            #check if the word stats with an alphabet\n",
    "            val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "            #ignore if it is a stop word\n",
    "            if(w in stopWords or val is None):\n",
    "                continue\n",
    "            else:\n",
    "                #processa plural\n",
    "                w = processa_plural(w)\n",
    "                featureVector.append(w.lower())\n",
    "\n",
    "    return featureVector\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_plural(word):\n",
    "    dict_plural ={'marques':'','rs':'','atras':'','aspas':'','apos':'','antes':'','alas':'','ademais':'','quantas':'','vamos':'','eses':'','esses':'','vezes':'','damos':'','queremos':'','apenas':'','duas':'','dois':'','atraves':'','caos':'','demais':'','ambos':'','detras':''}\n",
    "    if dict_plural.get(word)==None:\n",
    "        if re.search(r's$', word):\n",
    "\n",
    "            if re.search(r'as$', word):\n",
    "                word = re.sub(r'irmas$', 'irma', word)\n",
    "                word = re.sub(r'ais$', 'al', word)\n",
    "                word = re.sub(r'aos$', 'ao', word)\n",
    "                word = re.sub(r'as$', 'a', word)\n",
    "                \n",
    "            elif re.search(r'es$', word):\n",
    "                if re.search(r'maes$', word):\n",
    "                    word = 'maes'\n",
    "                elif re.search(r'bres$', word):\n",
    "                    word = re.sub(r'bres$', 'bre', word)\n",
    "                elif re.search(r'zes$', word):\n",
    "                    word = re.sub(r'zes$', 'z', word)\n",
    "                elif re.search(r'tres$', word):\n",
    "                    word = re.sub(r'tres$', 'tre', word)\n",
    "                elif re.search(r'ores$', word):\n",
    "                    word = re.sub(r'tres$', 'tre', word)\n",
    "                else:\n",
    "                    word = re.sub(r'oes$', 'ao', word)\n",
    "                    word = re.sub(r'aes$', 'ao', word)\n",
    "                    word = re.sub(r'gues$','gues', word)\n",
    "                    word = re.sub(r'res$', 'r', word)\n",
    "                    word = re.sub(r'es$', 'e', word)\n",
    "\n",
    "            elif re.search(r'is$', word):\n",
    "                if re.search(r'veis$', word):\n",
    "                    word = re.sub(r'veis$', 'vel', word)\n",
    "                else:\n",
    "                    word = re.sub(r'ais$', 'al', word)\n",
    "                    word = re.sub(r'zis$', 'zil', word)\n",
    "                    word = re.sub(r'eis$', 'il', word)\n",
    "\n",
    "            elif re.search(r'os$', word):\n",
    "                if re.search(r'emos$', word):\n",
    "                    word = re.sub(r'emos$', 'emos', word)\n",
    "                elif re.search(r'amos$', word):\n",
    "                    word = re.sub(r'amos$', 'amos', word)\n",
    "                elif re.search(r'armos$', word):\n",
    "                    word = re.sub(r'armos$', 'armos', word)\n",
    "                else:\n",
    "                    word = re.sub(r'ois$', 'oi', word)\n",
    "                    word = re.sub(r'os$', 'o', word)\n",
    "\n",
    "            elif re.search(r'us$', word):\n",
    "                word = re.sub(r'eus$', 'eu', word)\n",
    "                word = re.sub(r'us$', 'u', word)\n",
    "\n",
    "            else:\n",
    "                word = re.sub(r'ns$', 'm', word)\n",
    "\n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence):\n",
    "    sentence_words = set(sentence)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in sentence_words)\n",
    "    return features\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    d={}\n",
    "    excessoes = ['nao','tambem']\n",
    "    if len(words)>1:\n",
    "        for w in words:\n",
    "            if len(w)>1:\n",
    "                d['contains(%s)' % w]=True\n",
    "    elif len(words)==1:\n",
    "        if (words[0] in excessoes)==False:\n",
    "            d['contains(%s)' % words[0]]=True\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_base_treino():\n",
    "    conn = sqlite3.connect('tweets.sqlite')\n",
    "    cur = conn.cursor()\n",
    "    #cur.execute(\"SELECT * FROM tweet where tweet_id = '0' and text like '%não%inútil%'\")\n",
    "    cur.execute(\"SELECT reacao,sentimento FROM reacao where mark_for_train=1\")\n",
    "    base_treino = cur.fetchall()\n",
    "\n",
    "    for row in base_treino:\n",
    "        #print(row)\n",
    "        cur.execute(\"insert into tweet (tweet_id, text, sentiment) values ('0', '\" + row[0] + \"', '\" + row[1] + \"')\").fetchone()\n",
    "        conn.commit()\n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the sentence one by one and process it\n",
    "#inpTweets = csv.reader(open('BaseReLi2.csv', 'r'), delimiter=',')\n",
    "\n",
    "conn = sqlite3.connect('tweets.sqlite')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM tweet where tweet_id == '0'\")\n",
    "inpSentence = cur.fetchall()\n",
    "    \n",
    "stopWords = getStopWordList('portuguese')\n",
    "featureList = []\n",
    "featureVectorPositive=[]\n",
    "featureVectorNonPositive=[]\n",
    "\n",
    "# Get tweet words\n",
    "cont_neg=0\n",
    "cont_pos=0\n",
    "cont_neu=0\n",
    "cont_max=3000\n",
    "item_vec_max = 1\n",
    "\n",
    "for row in inpSentence:\n",
    "    \n",
    "    if len(row)==4:\n",
    "        if row[3] == 1 and cont_pos <= cont_max:\n",
    "            cont_pos +=1\n",
    "            sentiment = row[3]\n",
    "            sentence = row[2]\n",
    "            processedSentence = processSentence(sentence)\n",
    "            featureVector = getFeatureVector(processedSentence, stopWords)\n",
    "            if len(featureVector)>item_vec_max:\n",
    "                #tupla = list(nltk.bigrams(featureVector))\n",
    "                #for t in tupla:\n",
    "                    #featureVector.append(\"_\".join(\" \".join(t).split()))\n",
    "                featureVectorPositive.append(\" \".join(featureVector))\n",
    "                \n",
    "                1==1\n",
    "            else:\n",
    "                #print(featureVector, sentiment)\n",
    "                1==1\n",
    "\n",
    "        elif row[3] == -1 and cont_neg <= cont_max:\n",
    "            cont_neg +=1\n",
    "            sentiment = row[3]\n",
    "            sentence = row[2]\n",
    "            processedSentence = processSentence(sentence)\n",
    "            featureVector = getFeatureVector(processedSentence, stopWords)\n",
    "            if len(featureVector)>item_vec_max:\n",
    "                #tupla = list(nltk.bigrams(featureVector))\n",
    "                #for t in tupla:\n",
    "                    #featureVector.append(\"_\".join(\" \".join(t).split()))                    \n",
    "                featureVectorNonPositive.append(\" \".join(featureVector))\n",
    "                \n",
    "                1==1\n",
    "            else:\n",
    "                #print(featureVector, sentiment)\n",
    "                1==1\n",
    "\n",
    "        elif row[3] == 0 and cont_neu <= cont_max:\n",
    "            cont_neu +=1\n",
    "            sentiment = row[3]\n",
    "            sentence = row[2]\n",
    "            processedSentence = processSentence(sentence)\n",
    "            featureVector = getFeatureVector(processedSentence, stopWords)\n",
    "            if len(featureVector)>item_vec_max:\n",
    "                #tupla = list(nltk.bigrams(featureVector))\n",
    "                #for t in tupla:\n",
    "                    #featureVector.append(\"_\".join(\" \".join(t).split()))                    \n",
    "                featureVectorPositive.append(\" \".join(featureVector))\n",
    "                \n",
    "                1==1\n",
    "            else:\n",
    "                #print(featureVector, sentiment)\n",
    "                1==1\n",
    "conn.close()\n",
    "#end loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureVectorNonPositive = list(map(features, featureVectorNonPositive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureVectorPositive = list(map(features, featureVectorPositive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVP_train ,FVP_test = train_test_split(featureVectorPositive,random_state=24061976,shuffle=True,test_size=0.10)\n",
    "FVnP_train ,FVnP_test = train_test_split(featureVectorNonPositive,random_state=24061976,shuffle=True,test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import PositiveNaiveBayesClassifier\n",
    "classifier = PositiveNaiveBayesClassifier.train(featureVectorPositive,featureVectorNonPositive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jordana marques\n",
      "jordana marques\n",
      "{'contains(jordana)': True, 'contains(marques)': True}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "carrega_base_treino()\n",
    "minDicLen = 0\n",
    "test = 'Jordana Marques'\n",
    "processedTestTweet = processSentence(test)\n",
    "print(processedTestTweet)\n",
    "test = \" \".join((getFeatureVector(processedTestTweet,stopWords)))\n",
    "#tupla = list(nltk.bigrams(test.split()))\n",
    "\n",
    "#for t in tupla:\n",
    "    #test = test + \" \" + \"_\".join(\" \".join(t).split())\n",
    "        \n",
    "print(test)\n",
    "print(features(test))\n",
    "dic = features(test)\n",
    "if len(dic)>minDicLen:\n",
    "    print(classifier.classify(dic))\n",
    "else:\n",
    "    print(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.prob_classify(dic).prob(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.show_most_informative_features(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = []\n",
    "gold_result = []\n",
    "\n",
    "for i in range(0,len(FVnP_test)):\n",
    "    #test_result.append(classifier.classify(FVP_test[i])\n",
    "    test_result.append(str(classifier.classify(FVnP_test[i])))\n",
    "    gold_result.append('False')\n",
    "    if test_result != gold_result:\n",
    "        dict = {}\n",
    "        dict = FVnP_test[i]\n",
    "        list_key = list(dict.keys())\n",
    "        #print(\"Classificação: \" + 'True')\n",
    "        #print(\"Palavras\".center(70,\"*\"))\n",
    "        for c in range(0,len(list_key)):\n",
    "            if dict.get(list_key[c])==True:\n",
    "                #print(list_key[c]) \n",
    "                1==1\n",
    "                \n",
    "        #print(\"Fim\".center(70,\"*\"))\n",
    "\n",
    "CM = nltk.ConfusionMatrix(gold_result, test_result)\n",
    "print(CM)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'True', 'False'}\n",
    "\n",
    "from collections import Counter\n",
    "TP, FN, FP = Counter(), Counter(), Counter()\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            TP[i] += int(CM[i,j])\n",
    "        else:\n",
    "            FN[i] += int(CM[i,j])\n",
    "            FP[j] += int(CM[i,j])\n",
    "\n",
    "print(\"label\\tprecision\\trecall\\tf_measure\")\n",
    "for label in sorted(labels):\n",
    "    precision, recall = 0, 0\n",
    "    if TP[label] == 0:\n",
    "        f_measure = 0\n",
    "    else:\n",
    "        precision = float(TP[label]) / (TP[label]+FP[label])\n",
    "        recall = float(TP[label]) / (TP[label]+FN[label])\n",
    "        f_measure = float(2) * (precision * recall) / (precision + recall)\n",
    "    print(label+\"\\t\"+str(precision)+\"\\t\"+str(recall)+\"\\t\"+str(f_measure))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
